* Abstract
- The next challenge of game AI lies in Real Time Strategy (RTS) games.
/游戏AI的下一个战场在于实时策略游戏/
- RTS games provide partially observable gaming environments, where agents interact with one another in an action space much larger than that of GO. 
/RTS游戏提供了部分地观察视野，当代理与其他代理互动在行动空间中，此代理的活动空间多于GO/
- Mastering RTS games requires both strong macro strategies and delicate micro level execution. 
/操作RTS游戏要求不仅具有强大的宏观策略，而且具有熟练地微观层面操作/
- Recently, great progress has been made in micro level execution, while complete solutions for macro strategies are still lacking. 
/近期，微观层面操作有了巨大的进步，再此同时宏观策略的完整的解决方案仍然缺乏/
- In this paper, we propose a novel learning-based Hierarchical Macro Strategy model for mastering MOBA games, a sub-genre of RTS games. 
/在这个论文中，我们打算将新颖的基于学习分层宏观策略模型用于控制实时策略的一个领域——MOBA类游戏中/
- Trained by the Hierarchical Macro Strategy model, agents explicitly make macro strategy decisions and further guide their micro level execution. 
/通过训练这个分层宏观策略模型，代理清晰地在宏观策略中做出决定，并且更进一步地指导微观层面操作/
- Moreover, each of the agents makes independent strategy decisions, while simultaneously communicating with the allies through leveraging a novel imitated cross-agent communication mechanism. 
/此外，每一个代理都能独立的做出战略性决定，而且同时通过利用新颖的模仿反向代理交流机制与盟军进行交流/
- We perform comprehensive evaluations on a popular 5v5 Multiplayer Online Battle Arena (MOBA) game.
/我们在流行的5v5MOBA类游戏中得出了综合的评估/
- Our 5-AI team achieves a 48% winning rate against human player teams which are ranked top 1% in the player ranking system.
/我们的五AI队伍获得了48%的胜率在对抗人类玩家队伍，这些人类队伍在排名系统中位于1%/
* Introduction
- Light has been shed on artificial general intelligence after AlphaGo defeated world GO champion Lee Seedol (Silver et al. 2016). 
/在阿尔法GO击败世界围棋冠军李·西多尔后，通用人工智能的光芒已经散去。/
- Since then, game AI has drawn unprecedented attention from not only researchers but also the public. 
/从那时起，游戏AI引起了前所未有的关注，不仅仅是研究人员，还有公众。/
- Game AI aims much more than robots playing games. 
/游戏AI目的不只只是机器人玩游戏。/
- Rather, games provide ideal environments that simulate the real world.
/在某种程度上，游戏提供模拟现实世界的理想环境/
- AI researchers can conduct experiments in games, and transfer successful AI ability to the real world. 
/AI研究人员可以在游戏中进行实验，并且将成功的AI能力转换到现实世界。/
- Although AlphaGo is a milestone to the goal of general AI, the class of problems it represents is still simple compared to the real world. 
/尽管阿尔法GO是实现通用人工智能目标的一个里程碑，与现实世界相比，它所代表的问题类别仍然很简单。/
- Therefore, recently researchers have put much attention to real time strategy (RTS) games such as Defense of the Ancients (Dota) (OpenAI 2018a) and StarCraft (Vinyals et al. 2017; Tian et al. 2017), which represents a class of problems with next level complexity. 
/因此，最近研究者将注意力放在了实时战略游戏比如古代防御和星际争霸，它表示一类具有下一级复杂性的问题。/
- Dota is a famous set of science fiction 5v5 Multiplayer Online Battle Arena (MOBA) games. 
/古代防御是有名的一套科幻5v5多人在线竞技场游戏。/
- Each player controls one unit and cooperate with four allies to defend allies’ turrets, attack enemies’ turrets, collect resources by killing creeps, etc. The goal is to destroy enemies’ base.
/每一个玩家都控制一个单位，与四个盟友合作将保卫同盟的塔，攻击敌方的塔，通过击杀对手来收集资源，他们的目标是摧毁敌方基地。/
- There are four major aspects that make RTS games much more difficult compared to GO: 
/相比较于GO，有四个主要方面使得实时策略游戏更加复杂：/
- 1) Computational complexity.
/可计算复杂度。/
- The computational complexity in terms of action space or state space of RTS games can be up to 10^20,000, while the complexity of GO is about 10^250 (OpenAI 2018b).
/在RTS游戏的状态空间和行动空间，可计算复杂度超过10^20,000，而GO的复杂度大约在10^250/
- 2) Multi-agent.
/多智能体。/
- Playing RTS games usually involves multiple agents. It is crucial for multiple agents to coordinate and cooporate. 
/玩RTS游戏通常涉及到多个智能体。多个智能体之间的合作与协调是非常重要的。/
- 3) Imperfect information.
/不完美信息。/
- Different to GO, many RTS games make use of fog of war (Vinyals et al. 2017) to increase game uncertainty.
/不同于GO，许多的RTS游戏使用了战争迷雾来提升游戏的不确定性。/
- When the game map is not fully observable, it is essential to consider gaming among one another.
/当游戏地图不是完全可观测的，考虑彼此之间的博弈是很有必要的。/
- 4) Sparse and delayed rewards.
/稀疏和延时奖励。/
- Learning upon game rewards in GO is challenging because the rewards are usually sparse and delayed.
/在GO上学习游戏奖励是有挑战性的，因为奖赏通常是稀疏和延时的。/
- RTS game length could often be larger than 20,000 frames, while each GO game is usually no more than 361 steps. 
/RTS游戏的长度通常超过20,000帧，而游戏GO通常不到361步。/
- To master RTS games, players need to have strong skills in both macro strategy operation and micro level execution. 
/掌握RTS游戏，玩家需要有强大的技术在宏观策略操作和微观水平实施上。/
- In recent study, much attention and attempts have been put to micro level execution (Vinyals et al. 2017; Tian et al. 2017; Synnaeve and Bessiere 2011; Wender and Watson 2012). 
/在近期研究中，更多的关注和测试都是放在微观水平实施当中。/
- So far, Dota2 AI developed by OpenAI using reinforcement learning, i.e., OpenAI Five, has made the most advanced progress (OpenAI 2018a). 
/至今为止，基于增强学习的OPENAi开发的DOTA2的AI有了更多的进步。/
- OpenAI Five was trained directly on micro level action space using proximal policy optimization algorithms along with team rewards (Schulman et al. 2017). 
/OPENAIFive被直接训练在微观水平行动空间，用了近端策略优化算法随同团队奖赏。/
- OpenAI Five has shown strong teamfights skills and coordination comparable to top professional Dota2 teams during a demonstration match held in The International 2018 (DOTA2 2018). 
/在国际2018年举行的示范赛中，OPENAIFive表现出了与顶级专业DOTA2团队相当的强大团队战斗技能和协调能力。/
- OpenAI’s approach did not explicitly model macro strategy and tried to learn the entire game using micro level play. 
/OPENAI的研究没有明确地建模了宏观策略而是尝试学习使用微观水平玩整个游戏。/
- However, OpenAI Five was not able to defeat professional teams due to weakness in macro strategy management (Vincent 2018; Simonite 2018). 
/然而，OPENAIFive没有能够击败专业的队伍由于在宏观策略管理很弱。/
- Related work has also been done in explicit macro strategy operation, mostly focused on navigation. 
/在明确的宏观策略操作中，相关的工作已经完成，主要集中在导航方面。/
- Navigation aims to provide reasonable destination spots and efficient routes for agents. 
/导航的目的是为AI提供合理的目的地和有效的路线。/
- Most related work in navigation used influence maps or potential fields (DeLoura 2001; Hagelbäck and Johansson 2008; do Nascimento Silva and Chaimowicz 2015). 
/在导航中大多数相关工作使用的影响图或者势场。/
- Influence maps quantify units using handcrafted equations. 
/影响图使用手工制作的方程式量化单位。/
- Then, multiple influence maps are fused using rules to provide a single-value output to navigate agents.
/所以，多个影响图结合规则去提供一个单一价值的输出用来导航AI。/
- Providing destination is the most important purpose of navigation in terms of macro strategy operation.
/在一些宏观策略操作中，提供目的地是导航最重要的目的。/
- The ability to get to the right spots at right time makes essential difference between high level players and the others.
/在正确的时间到达正确的场地的能力在高水平玩家与普通玩家当中是至关重要的。/
- Planning has also been used in macro strategy operation. Ontanon et al.
/计划也被使用到宏观策略操作中。/
- proposed Adversarial Hierarchical-Task Network (AHTN) Planning (Ontanón and Buro 2015) to search hierarchical tasks in RTS game playing.
/提出了在RTS游戏中对抗性分层任务网络规划用来搜索分层任务。/
- Although AHTN shows promising results in a mini-RTS game, it suffers from efficiency issue which makes it difficult to apply to full MOBA games directly.
/尽管AHTN在迷你RTS游戏中显示出有希望的结果，但它存在效率问题，难以直接应用于完整的MOBA类游戏中。/
- Despite of the rich and promising literature, previous work in macro strategy failed to provide complete solution:
/尽管文献丰富，前景广阔，但以往的宏观战略研究未能提供完整的解决方案。/
- First, reasoning macro strategy implicitly by learning upon micro level action space may be too difficult.
/第一，通过学习微观水平行动空间获得隐含有意义的宏观策略可能太困难。/
- OpenAI Five’s ability gap between micro level execution and macro strategy operation was obvious.
/OPENAIFive的能力差距在微观水平实现与宏观策略操作之间是可观测的。/
- It might be over-optimistic to leave models to figure out high level strategies by simply looking at micro level actions and rewards.
/仅仅通过观察微观层面的行动和奖励，让模型去制定高层次的战略可能过于乐观。/
- We consider explicit macro strategy level modeling to be necessary.
/我们认为明确的宏观战略层面建模是必要的。/
- Second, previous work on explicit macro strategy heavily relied on handcrafted equations for influence maps/potential fields computation and fusion.
- In practice, there are usually thousands of numerical parameters to manually decide, which makes it nearly impossible to achieve good performance.
- Planning methods on the other hand cannot meet efficiency requirement of full MOBA games.
- Third, one of the most challenging problems in RTS game macro strategy operation is coordination among multiple agents.
- Nevertheless, to the best of our knowledge, previous work did not consider it in an explicit way.
- OpenAI Five considers multi-agent coordination using team rewards on micro level modeling.
- However, each agent of OpenAI Five makes decision without being aware of allies’ macro strategy decisions, making it difficult to develop top coordination ability in macro strategy level.
- Finally, we have found that modeling strategic phase is crucial for MOBA game AI performance.
- However, to the best of our knowledge, previous work did not consider this.
- Teaching agents to learn macro strategy operation, however, is challenging.
- Mathematically defining macro strategy, e.g., besiege and split push, is difficult in the first place.
- Also, incorporating macro strategy on top of OpenAI Five’s reinforcement learning framework (OpenAI 2018a) requires corresponding execution to gain rewards, while macro strategy execution is a complex ability to learn by itself.
- Therefore, we consider supervised learning to be a better scheme because high quality game replays can be fully leveraged to learn macro strategy along with corresponding execution samples.
- Note that macro strategy and execution learned using supervised learning can further act as an initial policy for reinforcement learning.
- In this paper, we propose Hierarchical Macro Strategy (HMS) model - a general supervised learning framework for MOBA games such as Dota.
- HMS directly tackles with computational complexity and multi-agent challenges of MOBA games.
- More specifically, HMS is a hierarchical model which conducts macro strategy operation by predicting attention on the game map under guidance of game phase modeling.
- Thereby, HMS reduces computational complexity by incorporating game knowledge.
- Moreover, each HMS agent conducts learning with a novel mechanism of communication with teammates agents to cope with multi-agent challenge.
- Finally, we have conducted extensive experiments in a popular MOBA game to evaluate our AI ability.
- We matched with hundreds of human player teams that ranked above 99% of players in the ranked system and achieved 48% winning rate.
- The rest of this paper is organized as follows:
- First, we briefly introduce Multiplayer Online Battle Arena (MOBA) games and compare the computational complexity with GO.
- Second, we illustrate our proposed Hierarchical Macro Strategy model.
- Then, we present experimental results in the fourth section.
- Finally, we conclude and discuss future work.

* Multiplayer Online Battle Arena (MOBA) Games
** Game Description
- MOBA is currently the most popular sub-genre of the RTS games.
- MOBA games are responsible for more than 30% of the online gameplay all over the world, with titles such as Dota, League of Legends, and Honour of Kings (Murphy 2015). 
- According to a worldwide digital games market report in February 2018, MOBA games ranked first in grossing in both PC and mobile games (SuperData 2018).
- In MOBA, the standard game mode requires two 5-player teams play against each other.
- Each player controls one unit, i.e., hero.
- There are numerous of heroes in MOBA, e.g., more than 80 in Honour of Kings.
- Each hero is uniquely designed with special characteristics and skills.
- Players control movement and skill releasing of heroes via the game interface.
 As shown in Figure. 1a, Honour of Kings players use left bottom steer button to control movements, while right bottom set of buttons to control skills.
- Surroundings are observable via the main screen.
- Players can also learn full map situation via the left top corner mini-map, where observable turrets, creeps, and heroes are displayed as thumbnails.
- Units are only observable either if they are allies’ units or if they are within a certain distance to allies’ units.
- There are three lanes of turrets for each team to defend, three turrets in each lane.
- There are also four jungle areas on the map, where creep resources can be collected to increase gold and experience.
- Each hero starts with minimum gold and level 1.
- Each team tries to leverage resources to obtain as much gold and experience as possible to purchase items and upgrade levels.
- The final goal is to destroy enemy’s base.
- A conceptual map of MOBA is shown in Figure. 1b.
- To master MOBA games, players need to have both excellent macro strategy operation and proficient micro level execution.
- Common macro strategies consist of opening, laning, ganking, ambushing, etc.
- Proficient micro level execution requires high accuracy of control and deep understanding of damage and effects of skills.
- Both macro strategy operation and micro level execution require mastery of timing to excel, which makes it extremely challenging and interesting.
- More discussion of MOBA can be found in (Silva and Chaimowicz 2017).
- Next, we will quantify the computational complexity of MOBA using Honour of Kings as an example.

** Computational Complexity
- The normal game length of Honour of Kings is about 20 minutes, i.e., approximately 20,000 frames in terms of gamecore.
- At each frame, players make decision with tens of options, including movement button with 24 directions, and a few skill buttons with corresponding releasing position/directions.
- Even with significant discretization and simplification, as well as reaction time increased to 200ms, the action space is at magnitude of 101,500.
- As for state space, the resolution of Honour of Kings map is 130,000 by 130,000 pixels, and the diameter of each unit is 1,000.
- At each frame, each unit may have different status such as hit points, levels, gold.
- Again, the state space is at magnitude of 1020,000 with significant simplification.
- Comparison of action space and state space between MOBA and GO is listed in Table. 1.

** MOBA AI Macro Strategy Architecture
- Our motivation of designing MOBA AI macro strategy model was inspired from how human players make strategic decisions.
- During MOBA games, experienced human players are fully aware of game phases, e.g., opening phase, laning phase, mid game phase, and late game phase (Silva and Chaimowicz 2017).
- During each phase, players pay attention to the game map and make corresponding decision on where to dispatch the heroes.
- For example, during the laning phase players tend to focus more on their own lanes rather than backing up allies, while during mid to late phases, players pay more attention to teamfight spots and pushing enemies’ base.
- To sum up, we formulate the macro strategy operation process as "phase recognition -> attention prediction -> execution".
- To model this process, we propose a two-layer macro strategy architecture, i.e., phase and attention:
- • Phase layer aims to recognize current game phase so that attention layer can have better sense about where to pay attention to.
- • Attention layer aims to predict the best region on game maps to dispatch heroes.
- Phase and Attention layers act as high level guidance for micro level execution.
- We will describe details of modeling in the next section.
- The network structure of micro level model is almost identical to the one used in OpenAI Five1 (OpenAI 2018a), but in a supervised learning manner.
- We did minor modification to adapt it to Honour of Kings, such as deleting Teleport.
* Hierarchical Macro Strategy Model
- We propose a Hierarchical Macro Strategy (HMS) model to consider both phase and attention layers in a unified neural network.
- We will first present the unified network architecture.
- Then, we illustrate how we construct each of the phase and attention layers.
** Model Overview
- We propose a Hierarchical Macro Strategy model (HMS) to model both attention and phase layers as a multi-task model.
- It takes game features as input.
- The output consists of two tasks, i.e., attention layer as the main task and phase layer as an auxiliary task.
- The output of attention layer directly conveys macro strategy embedding to micro level models, while resource layer acts as an axillary task which help refine the shared layers between attention and phase tasks.
- The illustrating network structure of HMS is listed in Figure. 2.
- HMS takes both image and vector features as input, carrying visual features and global features respectively.
- In image part, we use convolutional layers.
- In vector part, we use fully connected layers.
- The image and vector parts merge in two separate tasks, i.e., attention and phase.
- Ultimately, attention and phase tasks take input from shared layers through their own layers and output to compute loss.
** Attention Layer
- Similar to how players make decisions according to the game map, attention layer predicts the best region for agents to move to.
- However, it is tricky to tell from data that where is a player’s destination.
- We observe that regions where attack takes place can be indicator of players’ destination, because otherwise players would not have spent time on such spots.
- According to this observation, we define ground-truth regions as the regions where players conduct their next attack.
- An illustrating example is shown in Figure. 3.
- Let s to be one session in a game which contains several frames, and s − 1 indicates the session right before s.
 In Figure. 3, s − 1 is the first session in the game.
- Let ts to be the starting frame of s. Note that a session ends along with attack behavior, therefore there exists a region ys in ts where the hero conducts attack.
- As shown in Figure. 3, label for s−1 is ys, while label for s is ys+1.
- Intuitively, by setting up labels in this way, we expect agents to learn to move to ys at the beginning of game.
- Similarly, agents are supposed to move to appropriate regions given game situation.
** Phase layer
- Phase layer aims to recognize the current phase.
- Extracting game phases ground-truth is difficult because phase definition used by human players is abstract.
- Although roughly correlated to time, phases such as opening, laning, and late game depend on complicated judgment based on current game situation, which makes it difficult to extract groundtruth of game phases from replays.
- Fortunately, we observe clear correlation between game phases with major resources.
- For example, during the opening phase players usually aim at taking outer turrets and baron, while for late game, players operate to destroy enemies’ base.
- Therefore, we propose to model phases with respect to major resources.
- More specifically, major resources indicate turrets, baron, dragon, and base.
- We marked the major resources on the map in Figure. 4a.
- Label definition of phase layer is similar to attention layer.
- The only difference is that ys in phase layer indicates attack behavior on turrets, baron, dragon, and base instead of in regions.
- Intuitively, phase layer modeling splits the entire game into several phases via modeling which macro resource to take in current phase.
- We do not consider other resources such as lane creeps, heroes, and neutral creeps as major objectives because usually these resources are for bigger goal, such as destroying turrets or base with higher chance.
- Figure. 4b shows a series of attack behavior during the bottom outer turret strategy.
- The player killed two neutral creeps in the nearby jungle and several lane creeps in the bottom lane before attacking the bottom outer turret.
- We expect the model to learn when and what major resources to take given game situation, and in the meanwhile learn attention distribution that serve each of the major resources.
** Imitated Cross-agents Communication
- Cross-agents communication is essential for a team of agents to cooperate.
- There is rich literature of cross-agent communication on multi-agent reinforcement learning research (Sukhbaatar, Fergus, and others 2016; Foerster et al. 2016).
- However, it is challenging to learn communication using training data in supervised learning because the actual communication is unknown.
- To enable agents to communicate in supervised learning setting, we have designed a novel cross-agents communication mechanism.
- During training phase, we put attention labels of allies as features for training.
- During testing phase, we put attention prediction of allies as features and make decision correspondingly.
- In this way, our agents can "communicate" with one another and learn to cooperate upon allies’ decisions.
- We name this mechanism as Imitated Crossagents Communication due to its supervised nature.
* Experiments
- In this section, we evaluate our model performance.
- We first describe the experimental setup, including data preparation and model setup.
- Then, we present qualitative results such as attention distribution under different phase.
- Finally, we list the statistics of matches with human player teams and evaluate improvement brought by our proposed model.
** Experimental Setup
*** Data Preparation
- To train a model, we collect around 300 thousand game replays made of King Professional League competition and training records.
- Finally, 250 million instances were used for training.
- We consider both visual and attributes features.
- On visual side, we extract 85 features such as position and hit points of all units and then blur the visual features into 12*12 resolution.
- On attributes side, we extract 181 features such as roles of heroes, time period of game, hero ID, heroes’ gold and level status and Kill-DeathAssistance statistics.
*** Model Setup
- We use a mixture of convolutional and fully-connected layers to take inputs from visual and attributes features respectively.
- On convolutional side, we set five shared convolutional layers, each with 512 channels, padding = 1, and one RELU.
- Each of the tasks has two convolutional layers with exactly the same configuration with shared layers.
- On fully-connected layers side, we set two shared fully-connected layers with 512 nodes.
- Each of the tasks has two fully-connected layers with exactly the same configuration with shared layers.
- Then, we use one concatenation layer and two fully-connected layers to fuse results of convolutional layers and fully-connected layers.
- We use ADAM as the optimizer with base learning rate at 10e-6.
- Batch size was set at 128.
- The loss weights of both phase and attention tasks are set at 1.
- We used CAFFE (Jia et al. 2014) with eight GPU cards.
- The duration to train an HMS model was about 12 hours.
- Finally, the output for attention layer corresponds to 144 regions of the map, resolution of which is exactly the same as the visual inputs.
- The output of the phase task corresponds to 14 major resources circled in Figure. 4a.
** Experimental Results
*** Opening Attention
- Opening is one of the most important strategies in MOBA.
- We show one opening attention of different heroes learned by our model in Figure. 5.
- In Figure. 5, each subfigure consists of two square images.
- The lefthand-side square image indicates the attention distribution of the right-hand-side MOBA mini-map.
- The hottest region is highlighted with red circle.
- We list attention prediction of four heroes, i.e., Diaochan, Hanxin, Arthur, and Houyi.
- The four heroes belong to master, assasin, warrior, and archer respectively.
- According to the attention prediction, Diaochan is dispatched to middle lane, Hanxin will move to left jungle area, and Authur and Houyi will guard the bottom jungle area.
- The fifth hero Miyamoto Musashi, which was not plotted, will guard the top outer turret.
- This opening is considered safe and efficient, and widely used in Honour of Kings games.
*** Attention Distribution Affected by Phase Layer
- We visualize attention distribution of different phases in Figure. 6a and 6b.
- We can see that attention distributes around the major resource of each phase.
- For example, for upper outer turret phase in Figure. 6a, the attention distributes around upper outer region, as well as nearby jungle area.
- Also, as shown in Figure. 6b, attention distributes mainly in the middle lane, especially area in front of the base.
- These examples show that our phase layer modeling affects attention distribution in practice.
- To further examine how phase layer correlates with game phases, we conduct t-Distributed Stochastic Neighbor Embedding (t-SNE) on phase layer output.
- As shown in Figure. 7, samples are coloured with respect to different time stages.
- We can observe that samples are clearly separable with respect to time stages.
- For example, blue, orange and green (0-10 minuets) samples place close to one another, while red and purple samples (more than 10 minuets) form another group.
*** Macro Strategy Embedding
- We evaluate how important is the macro strategy modeling.
-We removed the macro strategy embedding and trained the model using micro level actions from the replays.
- The micro level model design is similar to OpenAI Five (OpenAI 2018a).
- Detail description of the micro level modeling is out of the scope of this paper.
- The result is listed in Table. 2, column AI Without Macro Strategy.
- As the result shows, HMS outperformed AI Without Macro Strategy with 75% winning rates.
- HMS performed much better than AI Without Macro Strategy in terms of number of kills, turrets destruction, and gold.
- The most obvious performance change is that AI Without Macro Strategy mainly focused on nearby targets.
- Agents did not care much about backing up teammates and pushing lane creeps in relatively large distance.
- They spent most of the time on killing neutral creeps and nearby lane creeps.
- The performance change can be observed from the comparison of engagement rate and number of turrets in Table. 2.
- This phenomenon may reflect how important macro strategy modeling is to highlight important spots.
*** Match against Human Players
- To evaluate our AI performance more accurately, we conduct matches between our AI and human players.
- We invited 250 human player teams whose average ranking is King in Honour of Kings rank system (above 1% of human players).
- Following the standard procedure of ranked match in Honour of Kings, we obey ban-pick rules to pick and ban heroes before each match.
- The ban-pick module was implemented using simple rules.
- Note that gamecores of Honour of Kings limit commands frequency to a level similar with human.
- The overall statistics are listed in Table. 2, column Human Teams.
- Our AI achieved 48% winning rate in the 250 games.
- The statistics show that our AI team did not have advantage on teamfight over human teams.
- The number of kills made by AI is about 15% less than human teams.
- Other items such as turrets destruction, engagement rate, and gold per minute were similar between AI and human.
- We have further observed that our AI destroyed 2.5 more turrets than human on average in the first 10 minutes.
- After 10 minutes, turrets difference shrank due to weaker teamfight ability compared to human teams.
- Arguably, our AI’s macro strategy operation ability is close to or above our human opponents.
*** Imitated Cross-agents Communication
- To evaluate how important the cross-agents communication mechanism is to the AI ability, we conduct matches between HMS and HMS trained without cross-agents communication.
- The result is listed in Table. 2, column AIWithout Communication.
- HMS achieved a 62.5% winning rate over the version without communication.
- We have observed obvious cross-agents cooperation learned when cross-agents communication was introduced.
- For example, rate of reasonable opening increased from 22% to 83% according to experts’ evaluation.
*** Phase layer
- We evaluate how phase layer affects the performance of HMS.
- We removed the phase layer and compared it with the full version of HMS.
- The result is listed in Table. 2, column AI Without phase layer.
- The result shows that phase layer modeling improved HMS significantly with 65% winning rate.
- We have also observed obvious AI ability downgrade when phase layer was removed.
- For example, agents were no longer accurate about timing when baron first appears, while the full version HMS agents got ready at 2:00 to gain baron as soon as possible.
* Conclusion and Future Work
- In this paper, we proposed a novel Hierarchical Macro Strategy model which models macro strategy operation for MOBA games.
/在此论文中，我们提议了一个新颖的分层宏观策略模型，对MOBA游戏的宏观策略操作进行建模/
- HMS explicitly models agents’ attention on game maps and considers game phase modeling.
/HMS清晰地代理人在游戏地图的注意力进行建模以及考虑游戏分阶段模型/
- We also proposed a novel imitated cross-agent communication mechanism which enables agents to cooperate.
/我们也提议了一个新颖的模仿反向代理交流机制使得代理能够合作/
- We used Honour of Kings as an example of MOBA games to implement and evaluate HMS.
/我们使用王者荣耀作为MOBA类游戏的一个例子去实践和评估HMS/
- We conducted matches between our AI and top 1% human player teams.
/我们在AI与人类1%顶尖队伍之间安排了比赛/
- Our AI achieves a 48% winning rate.
/我们的AI获得了48%胜率/
- To the best of our knowledge, our proposed HMS model is the first learning based model that explicitly models macro strategy for MOBA games.
/据我们所知，我们提议HMS模型是最好的基于学习的模型，这模型清楚地建模了宏观策略用于MOBA类游戏中/
- HMS used supervised learning to learn macro strategy operation and corresponding micro level execution from high quality replays.
/HMS使用监督学习从高质量重放中去训练宏观策略操作和相应的微观层面执行/
- A trained HMS model can be further used as an initial policy for reinforcement learning framework.
/训练过的HMS模型能够更好的使用作为最初的策略对于增强学习框架/
- Our proposed HMS model exhibits a strong potential in MOBA games.
/我们提出了HMS模型显示了非常强大的潜力在MOBA游戏中/
- It may be generalized to more RTS games with appropriate adaptations.
/这可能总结了RTS游戏，他对RTS游戏有着较好的适应性/
- For example, the attention layer modeling may be applicable to StarCraft, where the definition of attention can be extended to more meaningful behaviors such as building operation.
/例如，注意力层建模能够适应与星际争霸，注意力的限定能够使得更多有意义的行为例如建立操作/
- Also, Imitated Crossagents Communication can be used to learn to cooperate.
/除此之外，模仿反向代理通讯习惯于学习配合/
- Phase layer modeling is more game-specific.
/分阶段层建模更加具体于游戏/
- The resource collection procedure in StarCraft is different from that of MOBA, where gold is mined near the base.
/星际争霸的资源整合程序不同于其他的moba类游戏，他的黄金是离基地十分之近/
- Therefore, phase layer modeling may require game-specific design for different games.
/因此，对于不同的游戏，分阶段层建模可能需要具体游戏具体设计/
- However, the underlying idea to capture game phases can be generalized to Starcraft as well.
/然而，捕获游戏阶段的这个根本思想也能够广泛地用于星际争霸/
- HMS may also inspire macro strategy modeling in domains where multiple agents cooperate on a map and historical data is available.
/HMS可能也鼓舞了宏观策略建模领域，当在地图上多人代理合作且历史数据可使用/
- For example, in robot soccer, attention layer modeling and Imitated Cross-agents Communication may help robots position and cooperate given parsed soccer recordings.
/例如，在机器人足球中，注意力层建模和模仿反向代理通讯可能帮助机器人状况和合作去提供解析的足球记录/
- In the future, we will incorporate planning based on HMS.
/在未来，我们将在HMS的基础上纳入计划/
- Planning by MCTS roll-outs in Go has been proven essential to outperform top human players (Silver et al. 2016).
/通过在GO中的MCTS实现规划被证实对于击败顶尖人类玩家起到至关重要的一幕/
- We expect planning can be essential for RTS games as well, because it may not only be useful for imperfect information gaming but also be crucial to bringing in expected rewards which supervised learning fails to consider.
/我们期望此规划对于RTS游戏也能够起到至关重要,因为它不仅对于不完美信息游戏中有用，而且带来期望在监督学习欠考虑到的奖赏是至关重要的/

* References
[DeLoura 2001] DeLoura, M. A. 2001. Game programming gems 2. Cengage learning.

[do Nascimento Silva and Chaimowicz 2015] do Nascimento Silva, V., and Chaimowicz, L. 2015. On the development of intelligent agents for moba games. In Computer Games and Digital Entertainment (SBGames), 2015 14th Brazilian Symposium on, 142–151. IEEE. [DOTA2 2018] DOTA2. 2018. The international 2018. https://www.dota2.com/international/announcement/.

[Foerster et al. 2016] Foerster, J. N.; Assael, Y. M.; de Freitas, N.; and Whiteson, S. 2016. Learning to communicate to solve riddles with deep distributed recurrent q-networks. arXiv preprint arXiv:1602.02672.

[Hagelbäck and Johansson 2008] Hagelbäck, J., and Johansson, S. J. 2008. The rise of potential fields in real time strategy bots. In Fourth Artificial Intelligence and Interactive Digital Entertainment Conference. Stanford University.

[Jia et al. 2014] Jia, Y.; Shelhamer, E.; Donahue, J.; Karayev, S.; Long, J.; Girshick, R.; Guadarrama, S.; and Darrell, T. 2014. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093. [Murphy 2015] Murphy, M. 2015. Most played games: November 2015 – fallout 4 and black ops iii arise while starcraft ii shines. http://caas.raptr.com/most-played-gamesnovember-2015-fallout-4-andblack-ops-iii-arise-whilestarcraft-ii-shines/.

[Ontanón and Buro 2015] Ontanón, S., and Buro, M. 2015. Adversarial hierarchical-task network planning for complex real-time games. In Twenty-Fourth International Joint Conference on Artificial Intelligence.

[OpenAI 2018a] OpenAI. 2018a. Openai blog: Dota 2. https://blog.openai.com/dota-2/ (17 Apr 2018). [OpenAI 2018b] OpenAI.
2018b. Openai five. https://blog.openai.com/openai-five/ (25 Jun 2018).

[Schulman et al. 2017] Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and Klimov, O. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.

[Silva and Chaimowicz 2017] Silva, V. D. N., and Chaimowicz, L. 2017. Moba: a new arena for game ai. arXiv preprint arXiv:1705.10443.

[Silver et al. 2016] Silver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Sifre, L.; Van Den Driessche, G.; Schrittwieser, J.; Antonoglou, I.; Panneershelvam, V.; Lanctot, M.; et al. 2016. Mastering the game of go with deep neural networks and tree search. nature 529(7587):484–489.

[Simonite 2018] Simonite, T. 2018. Pro gamers fend off elon musk-backed ai bots—for now.
https://www.wired.com/story/pro-gamers-fend-off-elonmusks-ai-bots/ (Aug 23, 2018). [Sukhbaatar, Fergus, and others 2016] Sukhbaatar, S.; Fergus, R.; et al. 2016. Learning multiagent communication with backpropagation. In Advances in Neural Information Processing Systems, 2244–2252. [SuperData 2018] SuperData.
wide digital games market: February 2018. https://www.superdataresearch.com/us-digital-gamesmarket/.

[Synnaeve and Bessiere 2011] Synnaeve, G., and Bessiere, P. 2011. A bayesian model for rts units control applied to starcraft. In Computational Intelligence and Games (CIG), 2011 IEEE Conference on, 190–196. IEEE.

[Tian et al. 2017] Tian, Y.; Gong, Q.; Shang, W.; Wu, Y.; and Zitnick, C. L. 2017. Elf: An extensive, lightweight and flexible research platform for real-time strategy games. In Advances in Neural Information Processing Systems, 2656– 2666.

[Vincent 2018] Vincent, J. 2018. Humans grab victory in first of three dota 2 matches against openai. https://www.theverge.com/2018/8/23/17772376/openaidota-2-pain-game-human-victory-ai (Aug 23, 2018).

[Vinyals et al. 2017] Vinyals, O.; Ewalds, T.; Bartunov, S.; Georgiev, P.; Vezhnevets, A. S.; Yeo, M.; Makhzani, A.; Küttler, H.; Agapiou, J.; Schrittwieser, J.; et al. 2017. Starcraft ii: a new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782.

[Wender and Watson 2012] Wender, S., and Watson, I. 2012. Applying reinforcement learning to small scale combat in the real-time strategy game starcraft: Broodwar. In Computational Intelligence and Games (CIG), 2012 IEEE Conference on, 402–408. IEEE.
