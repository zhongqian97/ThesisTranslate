* Abstract
- The next challenge of game AI lies in Real Time Strategy (RTS) games.
/游戏AI的下一个战场在于实时策略游戏/
- RTS games provide partially observable gaming environments, where agents interact with one another in an action space much larger than that of GO. 
/RTS游戏提供了部分地观察视野，当代理与其他代理互动在行动空间中，此代理的活动空间多于GO/
- Mastering RTS games requires both strong macro strategies and delicate micro level execution. 
/操作RTS游戏要求不仅具有强大的宏观策略，而且具有熟练地微观层面操作/
- Recently, great progress has been made in micro level execution, while complete solutions for macro strategies are still lacking. 
/近期，微观层面操作有了巨大的进步，再此同时宏观策略的完整的解决方案仍然缺乏/
- In this paper, we propose a novel learning-based Hierarchical Macro Strategy model for mastering MOBA games, a sub-genre of RTS games. 
/在这个论文中，我们打算将新颖的基于学习分层宏观策略模型用于控制实时策略的一个领域——MOBA类游戏中/
- Trained by the Hierarchical Macro Strategy model, agents explicitly make macro strategy decisions and further guide their micro level execution. 
/通过训练这个分层宏观策略模型，代理清晰地在宏观策略中做出决定，并且更进一步地指导微观层面操作/
- Moreover, each of the agents makes independent strategy decisions, while simultaneously communicating with the allies through leveraging a novel imitated cross-agent communication mechanism. 
/此外，每一个代理都能独立的做出战略性决定，而且同时通过利用新颖的模仿反向代理交流机制与盟军进行交流/
- We perform comprehensive evaluations on a popular 5v5 Multiplayer Online Battle Arena (MOBA) game.
/我们在流行的5v5MOBA类游戏中得出了综合的评估/
- Our 5-AI team achieves a 48% winning rate against human player teams which are ranked top 1% in the player ranking system.
/我们的五AI队伍获得了48%的胜率在对抗人类玩家队伍，这些人类队伍在排名系统中位于1%/
* Introduction
- Light has been shed on artificial general intelligence after AlphaGo defeated world GO champion Lee Seedol (Silver et al. 2016). 
- Since then, game AI has drawn unprecedented attention from not only researchers but also the public. 
- Game AI aims much more than robots playing games. 
- Rather, games provide ideal environments that simulate the real world.
- AI researchers can conduct experiments in games, and transfer successful AI ability to the real world. 
- Although AlphaGo is a milestone to the goal of general AI, the class of problems it represents is still simple compared to the real world. 
- Therefore, recently researchers have put much attention to real time strategy (RTS) games such as Defense of the Ancients (Dota) (OpenAI 2018a) and StarCraft (Vinyals et al. 2017; Tian et al. 2017), which represents a class of problems with next level complexity. 
- Dota is a famous set of science fiction 5v5 Multiplayer Online Battle Arena (MOBA) games. 
- Each player controls one unit and cooperate with four allies to defend allies’ turrets, attack enemies’ turrets, collect resources by killing creeps, etc. The goal is to destroy enemies’ base.
- There are four major aspects that make RTS games much more difficult compared to GO: 
- 1) Computational complexity.
- The computational complexity in terms of action space or state space of RTS games can be up to 1020,000, while the complexity of GO is about 10250 (OpenAI 2018b).
- 2) Multi-agent.
- Playing RTS games usually involves multiple agents. It is crucial for multiple agents to coordinate and cooporate. 
- 3) Imperfect information.
- Different to GO, many RTS games make use of fog of war (Vinyals et al. 2017) to increase game uncertainty.
- When the game map is not fully observable, it is essential to consider gaming among one another.
- 4) Sparse and delayed rewards.
- Learning upon game rewards in GO is challenging because the rewards are usually sparse and delayed.
- RTS game length could often be larger than 20,000 frames, while each GO game is usually no more than 361 steps. 
- To master RTS games, players need to have strong skills in both macro strategy operation and micro level execution. 
- In recent study, much attention and attempts have been put to micro level execution (Vinyals et al. 2017; Tian et al. 2017; Synnaeve and Bessiere 2011; Wender and Watson 2012). 
- So far, Dota2 AI developed by OpenAI using reinforcement learning, i.e., OpenAI Five, has made the most advanced progress (OpenAI 2018a). 
- OpenAI Five was trained directly on micro level action space using proximal policy optimization algorithms along with team rewards (Schulman et al. 2017). 
- OpenAI Five has shown strong teamfights skills and coordination comparable to top professional Dota2 teams during a demonstration match held in The International 2018 (DOTA2 2018). 
- OpenAI’s approach did not explicitly model macro strategy and tried to learn the entire game using micro level play. 
- However, OpenAI Five was not able to defeat professional teams due to weakness in macro strategy management (Vincent 2018; Simonite 2018). 
- Related work has also been done in explicit macro strategy operation, mostly focused on navigation. 
- Navigation aims to provide reasonable destination spots and efficient routes for agents. 
- Most related work in navigation used influence maps or potential fields (DeLoura 2001; Hagelbäck and Johansson 2008; do Nascimento Silva and Chaimowicz 2015). 
- Influence maps quantify units using handcrafted equations. 
- Then, multiple influence maps are fused using rules to provide a single-value output to navigate agents.
- Providing destination is the most important purpose of navigation in terms of macro strategy operation.
- The ability to get to the right spots at right time makes essential difference between high level players and the others.
- Planning has also been used in macro strategy operation.
- Ontanon et al. proposed Adversarial Hierarchical-Task Network (AHTN) Planning (Ontanón and Buro 2015) to search hierarchical tasks in RTS game playing.
- Although AHTN shows promising results in a mini-RTS game, it suffers from efficiency issue which makes it difficult to apply to full MOBA games directly.
- Despite of the rich and promising literature, previous work in macro strategy failed to provide complete solution:
- First, reasoning macro strategy implicitly by learning upon micro level action space may be too difficult.
- OpenAI Five’s ability gap between micro level execution and macro strategy operation was obvious.
- It might be over-optimistic to leave models to figure out high level strategies by simply looking at micro level actions and rewards.
- We consider explicit macro strategy level modeling to be necessary.
- Second, previous work on explicit macro strategy heavily relied on handcrafted equations for influence maps/potential fields computation and fusion.
- In practice, there are usually thousands of numerical parameters to manually decide, which makes it nearly impossible to achieve good performance.
- Planning methods on the other hand cannot meet efficiency requirement of full MOBA games.
- Third, one of the most challenging problems in RTS game macro strategy operation is coordination among multiple agents.
- Nevertheless, to the best of our knowledge, previous work did not consider it in an explicit way.
- OpenAI Five considers multi-agent coordination using team rewards on micro level modeling.
- However, each agent of OpenAI Five makes decision without being aware of allies’ macro strategy decisions, making it difficult to develop top coordination ability in macro strategy level.
- Finally, we have found that modeling strategic phase is crucial for MOBA game AI performance.
- However, to the best of our knowledge, previous work did not consider this.
- Teaching agents to learn macro strategy operation, however, is challenging.
- Mathematically defining macro strategy, e.g., besiege and split push, is difficult in the first place.
- Also, incorporating macro strategy on top of OpenAI Five’s reinforcement learning framework (OpenAI 2018a) requires corresponding execution to gain rewards, while macro strategy execution is a complex ability to learn by itself.
- Therefore, we consider supervised learning to be a better scheme because high quality game replays can be fully leveraged to learn macro strategy along with corresponding execution samples.
- Note that macro strategy and execution learned using supervised learning can further act as an initial policy for reinforcement learning.
- In this paper, we propose Hierarchical Macro Strategy (HMS) model - a general supervised learning framework for MOBA games such as Dota.
- HMS directly tackles with computational complexity and multi-agent challenges of MOBA games.
- More specifically, HMS is a hierarchical model which conducts macro strategy operation by predicting attention on the game map under guidance of game phase modeling.
- Thereby, HMS reduces computational complexity by incorporating game knowledge.
- Moreover, each HMS agent conducts learning with a novel mechanism of communication with teammates agents to cope with multi-agent challenge.
- Finally, we have conducted extensive experiments in a popular MOBA game to evaluate our AI ability.
- We matched with hundreds of human player teams that ranked above 99% of players in the ranked system and achieved 48% winning rate.
- The rest of this paper is organized as follows:
- First, we briefly introduce Multiplayer Online Battle Arena (MOBA) games and compare the computational complexity with GO.
- Second, we illustrate our proposed Hierarchical Macro Strategy model.
- Then, we present experimental results in the fourth section.
- Finally, we conclude and discuss future work.
* Multiplayer Online Battle Arena (MOBA) Games
** Game Description
** Computational Complexity
** MOBA AI Macro Strategy Architecture
* Hierarchical Macro Strategy Model
** Model Overview
** Attention Layer
** Phase layer
** Imitated Cross-agents Communication
* Experiments
** Experimental Setup
** Experimental Results
* Conclusion and Future Work
- In this paper, we proposed a novel Hierarchical Macro Strategy model which models macro strategy operation for MOBA games.
/在此论文中，我们提议了一个新颖的分层宏观策略模型，对MOBA游戏的宏观策略操作进行建模/
- HMS explicitly models agents’ attention on game maps and considers game phase modeling.
/HMS清晰地代理人在游戏地图的注意力进行建模以及考虑游戏分阶段模型/
- We also proposed a novel imitated cross-agent communication mechanism which enables agents to cooperate.
/我们也提议了一个新颖的模仿反向代理交流机制使得代理能够合作/
- We used Honour of Kings as an example of MOBA games to implement and evaluate HMS.
/我们使用王者荣耀作为MOBA类游戏的一个例子去实践和评估HMS/
- We conducted matches between our AI and top 1% human player teams.
/我们在AI与人类1%顶尖队伍之间安排了比赛/
- Our AI achieves a 48% winning rate.
/我们的AI获得了48%胜率/
- To the best of our knowledge, our proposed HMS model is the first learning based model that explicitly models macro strategy for MOBA games.
/据我们所知，我们提议HMS模型是最好的基于学习的模型，这模型清楚地建模了宏观策略用于MOBA类游戏中/
- HMS used supervised learning to learn macro strategy operation and corresponding micro level execution from high quality replays.
/HMS使用监督学习从高质量重放中去训练宏观策略操作和相应的微观层面执行/
- A trained HMS model can be further used as an initial policy for reinforcement learning framework.
/训练过的HMS模型能够更好的使用作为最初的策略对于增强学习框架/
- Our proposed HMS model exhibits a strong potential in MOBA games.
/我们提出了HMS模型显示了非常强大的潜力在MOBA游戏中/
- It may be generalized to more RTS games with appropriate adaptations.
- For example, the attention layer modeling may be applicable to StarCraft, where the definition of attention can be extended to more meaningful behaviors such as building operation.
- Also, Imitated Crossagents Communication can be used to learn to cooperate.
- Phase layer modeling is more game-specific.
- The resource collection procedure in StarCraft is different from that of MOBA, where gold is mined near the base.
- Therefore, phase layer modeling may require game-specific design for different games.
- However, the underlying idea to capture game phases can be generalized to Starcraft as well.
- HMS may also inspire macro strategy modeling in domains where multiple agents cooperate on a map and historical data is available.
- For example, in robot soccer, attention layer modeling and Imitated Cross-agents Communication may help robots position and cooperate given parsed soccer recordings.
- In the future, we will incorporate planning based on HMS.
- Planning by MCTS roll-outs in Go has been proven essential to outperform top human players (Silver et al. 2016).
- We expect planning can be essential for RTS games as well, because it may not only be useful for imperfect information gaming but also be crucial to bringing in expected rewards which supervised learning fails to consider.

* References
[DeLoura 2001] DeLoura, M. A. 2001. Game programming gems 2. Cengage learning.

[do Nascimento Silva and Chaimowicz 2015] do Nascimento Silva, V., and Chaimowicz, L. 2015. On the development of intelligent agents for moba games. In Computer Games and Digital Entertainment (SBGames), 2015 14th Brazilian Symposium on, 142–151. IEEE. [DOTA2 2018] DOTA2. 2018. The international 2018. https://www.dota2.com/international/announcement/.

[Foerster et al. 2016] Foerster, J. N.; Assael, Y. M.; de Freitas, N.; and Whiteson, S. 2016. Learning to communicate to solve riddles with deep distributed recurrent q-networks. arXiv preprint arXiv:1602.02672.

[Hagelbäck and Johansson 2008] Hagelbäck, J., and Johansson, S. J. 2008. The rise of potential fields in real time strategy bots. In Fourth Artificial Intelligence and Interactive Digital Entertainment Conference. Stanford University.

[Jia et al. 2014] Jia, Y.; Shelhamer, E.; Donahue, J.; Karayev, S.; Long, J.; Girshick, R.; Guadarrama, S.; and Darrell, T. 2014. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093. [Murphy 2015] Murphy, M. 2015. Most played games: November 2015 – fallout 4 and black ops iii arise while starcraft ii shines. http://caas.raptr.com/most-played-gamesnovember-2015-fallout-4-andblack-ops-iii-arise-whilestarcraft-ii-shines/.

[Ontanón and Buro 2015] Ontanón, S., and Buro, M. 2015. Adversarial hierarchical-task network planning for complex real-time games. In Twenty-Fourth International Joint Conference on Artificial Intelligence.

[OpenAI 2018a] OpenAI. 2018a. Openai blog: Dota 2. https://blog.openai.com/dota-2/ (17 Apr 2018). [OpenAI 2018b] OpenAI.
2018b. Openai five. https://blog.openai.com/openai-five/ (25 Jun 2018).

[Schulman et al. 2017] Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and Klimov, O. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.

[Silva and Chaimowicz 2017] Silva, V. D. N., and Chaimowicz, L. 2017. Moba: a new arena for game ai. arXiv preprint arXiv:1705.10443.

[Silver et al. 2016] Silver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Sifre, L.; Van Den Driessche, G.; Schrittwieser, J.; Antonoglou, I.; Panneershelvam, V.; Lanctot, M.; et al. 2016. Mastering the game of go with deep neural networks and tree search. nature 529(7587):484–489.

[Simonite 2018] Simonite, T. 2018. Pro gamers fend off elon musk-backed ai bots—for now.
https://www.wired.com/story/pro-gamers-fend-off-elonmusks-ai-bots/ (Aug 23, 2018). [Sukhbaatar, Fergus, and others 2016] Sukhbaatar, S.; Fergus, R.; et al. 2016. Learning multiagent communication with backpropagation. In Advances in Neural Information Processing Systems, 2244–2252. [SuperData 2018] SuperData.
wide digital games market: February 2018. https://www.superdataresearch.com/us-digital-gamesmarket/.

[Synnaeve and Bessiere 2011] Synnaeve, G., and Bessiere, P. 2011. A bayesian model for rts units control applied to starcraft. In Computational Intelligence and Games (CIG), 2011 IEEE Conference on, 190–196. IEEE.

[Tian et al. 2017] Tian, Y.; Gong, Q.; Shang, W.; Wu, Y.; and Zitnick, C. L. 2017. Elf: An extensive, lightweight and flexible research platform for real-time strategy games. In Advances in Neural Information Processing Systems, 2656– 2666.

[Vincent 2018] Vincent, J. 2018. Humans grab victory in first of three dota 2 matches against openai. https://www.theverge.com/2018/8/23/17772376/openaidota-2-pain-game-human-victory-ai (Aug 23, 2018).

[Vinyals et al. 2017] Vinyals, O.; Ewalds, T.; Bartunov, S.; Georgiev, P.; Vezhnevets, A. S.; Yeo, M.; Makhzani, A.; Küttler, H.; Agapiou, J.; Schrittwieser, J.; et al. 2017. Starcraft ii: a new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782.

[Wender and Watson 2012] Wender, S., and Watson, I. 2012. Applying reinforcement learning to small scale combat in the real-time strategy game starcraft: Broodwar. In Computational Intelligence and Games (CIG), 2012 IEEE Conference on, 402–408. IEEE.
